---
title: 'Principal Component Analysis in R for RNA-sequencing data'
date: 2052-02-24
permalink: /posts/2022/01/corr-PCA-post/
tags:
  - statistics
  - principal component analysis
  - R
---

### Introduction

Principal component analysis is often one of the first steps involved in the analysis of high dimensional data. These types of data have...well...a lot of dimensions, possibly hundreds or maybe thousands. And this is part of what makes analyzing this data so daunting at first glance.

RNA sequencing data is a good example of such daunting data. The human genome has roughly 20,000 genes, and depending on the cell type you are looking at a good portion of these genes might be detected by RNAseq. Often this is returned as individual rows of a matrix, each of which could be expressed at varying levels in an individual sample (represented by normalized count values that are organized into columns representing a single sample). Looking at each one these 20k rows across all the samples in your data set just isn't feasible sometimes. So what do you do?

### Dimensionality Reduction

There are several reasons that one of the first things you might want to do with your RNA-sequencing data is to reduce the number of dimensions using something like principal component analysis.

You may want to:

* Look for technical outliers
* Quickly assess the within-group variance of experimental replicates (i.e do biologically similar samples group together?)
* Quantify how much variation in your data set can be explained by technical factors (such as sequencing depth or batches)

I'm not going to go into the theory behind the linear algebra that is running under the hood of the different dim reduction techniques here, but instead I'll leave you with an analogy that I really like.

#### Coke Can Analogy

First, imagine you grew up on another planet. In fact let's say it's in a universe where your species has only the developed the ability to see the world in 2 dimensions at a time. Length and width. That's it. Everything you've ever experienced could be perfectly drawn on a piece of paper, if you knew what that was. We'll call this world flatworld.

Suddenly a wormhole opens up in the sky and drops a single mysterious object into your world: A single, fully 3 dimensional can of coke. Obviously you and the other brightest minds of flatworld have been tasked with figuring out as much as you can about this alien object. With your limited perception you can see that this cylindrical can seems to be made up of an endless amount of circles stacked on top of each other. But you're clever, you notice that on the outside of this can are what appear to be markings, changes in coloring, and lines which might have some sort of particular meaning...but you can only see each one at a time and it's impossible to make out the pattern. So how do you see what is written on the outside of this can when you can only look at it from a top-down view??

We'll the best way could be to reduce the number of dimensions of the object until it fits within a framework your mind can understand (i.e. 2 dimensions). So you tell your colleagues to start cutting. They use their 2D scissors to cut the stacked circles on a **perpendicular** (or orthogonal) axis to the one they are stacked on until finally you can completely flatten out the entire edge. Now, reduced to only two dimensions, you can make out the classic red coca cola logo as well as many other mysterious writings that appear as patterns which until now had been hidden in the dimensions you couldn't perceive.

What the flatworlders have effectively done is PCA, although instead of using scissors PCA uses math (something called eigenvectors) to find the most "informative" axis to cut the coke can along. We humans can perceive up to 3 dimensions at a time so, while we wouldn't need to resort to fancy maths to read a coke can, dimensional reduction techniques come in handy for quickly and intuitively interpreting datasets with MANY more dimensions than 3.

### Normalization

### PCA

### Interpreting Results

### Correlating meta-data variables with latent variables

### Corrections (batch)
